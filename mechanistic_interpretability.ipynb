{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ac5f187",
   "metadata": {},
   "source": [
    "# Mechanistic Interpretability: Neuron Interactions\n",
    "\n",
    "This notebook explores how neurons interact with each other and what computational roles they play in the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90696ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebaddb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNISTNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "    def get_layer_activations(self, x, layer_name):\n",
    "        x = x.view(-1, 784)\n",
    "        activations = {}\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        if layer_name == 'fc1':\n",
    "            activations['fc1'] = x.clone()\n",
    "\n",
    "        x = F.relu(self.fc2(x))\n",
    "        if layer_name == 'fc2':\n",
    "            activations['fc2'] = x.clone()\n",
    "\n",
    "        x = F.relu(self.fc3(x))\n",
    "        if layer_name == 'fc3':\n",
    "            activations['fc3'] = x.clone()\n",
    "\n",
    "        x = self.fc4(x)\n",
    "        if layer_name == 'fc4':\n",
    "            activations['fc4'] = x.clone()\n",
    "\n",
    "        return activations.get(layer_name, None)\n",
    "    \n",
    "    def get_all_activations(self, x):\n",
    "        \"\"\"Get activations from all layers\"\"\"\n",
    "        x = x.view(-1, 784)\n",
    "        activations = {}\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        activations['fc1'] = x.clone()\n",
    "        \n",
    "        x = F.relu(self.fc2(x))\n",
    "        activations['fc2'] = x.clone()\n",
    "        \n",
    "        x = F.relu(self.fc3(x))\n",
    "        activations['fc3'] = x.clone()\n",
    "        \n",
    "        x = self.fc4(x)\n",
    "        activations['fc4'] = x.clone()\n",
    "        \n",
    "        return activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce5254f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST data\n",
    "transform = transforms.ToTensor()\n",
    "train_set = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_set = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130509bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, epochs=5):\n",
    "    \"\"\"Train the MNIST model\"\"\"\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{epochs}, Average Loss: {total_loss/len(train_loader):.4f}')\n",
    "    \n",
    "    print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5f0dae",
   "metadata": {},
   "source": [
    "## Neuron Correlation Analysis\n",
    "\n",
    "Analyze how neurons co-activate to understand their functional relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8b5f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_neuron_correlations(model, data_loader, layer_name='fc2', max_samples=1000):\n",
    "    \"\"\"Analyze how neurons co-activate to understand their interactions\"\"\"\n",
    "    model.eval()\n",
    "    all_activations = []\n",
    "    \n",
    "    sample_count = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in data_loader:\n",
    "            if sample_count >= max_samples:\n",
    "                break\n",
    "            \n",
    "            activations = model.get_layer_activations(data, layer_name)\n",
    "            all_activations.append(activations)\n",
    "            sample_count += data.size(0)\n",
    "    \n",
    "    # Concatenate all activations\n",
    "    all_activations = torch.cat(all_activations, dim=0)\n",
    "    \n",
    "    # Compute correlation matrix\n",
    "    correlation_matrix = torch.corrcoef(all_activations.T).cpu().numpy()\n",
    "    \n",
    "    # Plot correlation heatmap\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(correlation_matrix, cmap='RdBu_r', center=0, \n",
    "                vmin=-1, vmax=1, square=True)\n",
    "    plt.title(f'Neuron-Neuron Correlations ({layer_name})')\n",
    "    plt.xlabel('Neuron Index')\n",
    "    plt.ylabel('Neuron Index')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return correlation_matrix, all_activations\n",
    "\n",
    "def find_neuron_clusters(correlation_matrix, threshold=0.6):\n",
    "    \"\"\"Find groups of highly correlated neurons\"\"\"\n",
    "    high_corr_pairs = []\n",
    "    n_neurons = correlation_matrix.shape[0]\n",
    "    \n",
    "    for i in range(n_neurons):\n",
    "        for j in range(i+1, n_neurons):\n",
    "            if abs(correlation_matrix[i, j]) > threshold:\n",
    "                high_corr_pairs.append((i, j, correlation_matrix[i, j]))\n",
    "    \n",
    "    print(f\"Found {len(high_corr_pairs)} neuron pairs with |correlation| > {threshold}\")\n",
    "    \n",
    "    # Show top 10 most correlated pairs\n",
    "    high_corr_pairs.sort(key=lambda x: abs(x[2]), reverse=True)\n",
    "    print(\"\\nTop 10 most correlated neuron pairs:\")\n",
    "    for i, (n1, n2, corr) in enumerate(high_corr_pairs[:10]):\n",
    "        print(f\"{i+1}. Neurons {n1}-{n2}: correlation = {corr:.3f}\")\n",
    "    \n",
    "    return high_corr_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb77b6a",
   "metadata": {},
   "source": [
    "## Neuron Clustering and Functional Groups\n",
    "\n",
    "Use clustering to identify functional groups of neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3182da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_neurons_by_activity(activations, n_clusters=8):\n",
    "    \"\"\"Cluster neurons based on their activation patterns\"\"\"\n",
    "    # Use neuron activations across all samples as features\n",
    "    neuron_profiles = activations.T.cpu().numpy()  # Shape: (n_neurons, n_samples)\n",
    "    \n",
    "    # Apply K-means clustering\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    cluster_labels = kmeans.fit_predict(neuron_profiles)\n",
    "    \n",
    "    return cluster_labels, kmeans\n",
    "\n",
    "def visualize_neuron_clusters(activations, cluster_labels, n_clusters):\n",
    "    \"\"\"Visualize neuron clusters using PCA\"\"\"\n",
    "    # Reduce dimensionality for visualization\n",
    "    pca = PCA(n_components=2)\n",
    "    neuron_profiles_2d = pca.fit_transform(activations.T.cpu().numpy())\n",
    "    \n",
    "    # Plot clusters\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, n_clusters))\n",
    "    \n",
    "    for cluster_id in range(n_clusters):\n",
    "        mask = cluster_labels == cluster_id\n",
    "        plt.scatter(neuron_profiles_2d[mask, 0], neuron_profiles_2d[mask, 1], \n",
    "                   c=[colors[cluster_id]], label=f'Cluster {cluster_id}', \n",
    "                   alpha=0.7, s=50)\n",
    "    \n",
    "    plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)')\n",
    "    plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)')\n",
    "    plt.title('Neuron Clusters (PCA Visualization)')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return pca\n",
    "\n",
    "def analyze_cluster_specialization(model, data_loader, cluster_labels, layer_name='fc2'):\n",
    "    \"\"\"Analyze what each cluster specializes in\"\"\"\n",
    "    model.eval()\n",
    "    n_clusters = len(np.unique(cluster_labels))\n",
    "    \n",
    "    # Collect activations for each digit and cluster\n",
    "    cluster_digit_activations = {cluster_id: {digit: [] for digit in range(10)} \n",
    "                               for cluster_id in range(n_clusters)}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in data_loader:\n",
    "            activations = model.get_layer_activations(data, layer_name)\n",
    "            \n",
    "            for digit in range(10):\n",
    "                digit_mask = (target == digit)\n",
    "                if digit_mask.sum() > 0:\n",
    "                    digit_activations = activations[digit_mask]\n",
    "                    \n",
    "                    # Average activations for each cluster\n",
    "                    for cluster_id in range(n_clusters):\n",
    "                        cluster_mask = cluster_labels == cluster_id\n",
    "                        cluster_neurons = digit_activations[:, cluster_mask]\n",
    "                        if cluster_neurons.size(1) > 0:\n",
    "                            avg_activation = cluster_neurons.mean().item()\n",
    "                            cluster_digit_activations[cluster_id][digit].append(avg_activation)\n",
    "    \n",
    "    # Compute average activations\n",
    "    cluster_specialization = np.zeros((n_clusters, 10))\n",
    "    for cluster_id in range(n_clusters):\n",
    "        for digit in range(10):\n",
    "            if cluster_digit_activations[cluster_id][digit]:\n",
    "                cluster_specialization[cluster_id, digit] = np.mean(\n",
    "                    cluster_digit_activations[cluster_id][digit])\n",
    "    \n",
    "    # Visualize specialization\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(cluster_specialization, annot=True, fmt='.2f', \n",
    "                xticklabels=[f'Digit {i}' for i in range(10)],\n",
    "                yticklabels=[f'Cluster {i}' for i in range(n_clusters)],\n",
    "                cmap='viridis')\n",
    "    plt.title('Cluster Specialization by Digit')\n",
    "    plt.xlabel('Digit Class')\n",
    "    plt.ylabel('Neuron Cluster')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Find most specialized clusters\n",
    "    print(\"\\nCluster specialization analysis:\")\n",
    "    for cluster_id in range(n_clusters):\n",
    "        cluster_size = np.sum(cluster_labels == cluster_id)\n",
    "        best_digit = np.argmax(cluster_specialization[cluster_id])\n",
    "        specialization_score = cluster_specialization[cluster_id, best_digit]\n",
    "        print(f\"Cluster {cluster_id} ({cluster_size} neurons): Most active for digit {best_digit} (activation: {specialization_score:.3f})\")\n",
    "    \n",
    "    return cluster_specialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91304c93",
   "metadata": {},
   "source": [
    "## Pairwise Neuron Interaction Analysis\n",
    "\n",
    "Study how pairs of neurons interact and influence each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad7f405",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_neuron_interactions(model, data_loader, layer_name='fc2', target_digit=7):\n",
    "    \"\"\"Analyze how neurons interact for specific digit recognition\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Collect activations for target digit\n",
    "    target_activations = []\n",
    "    other_activations = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in data_loader:\n",
    "            activations = model.get_layer_activations(data, layer_name)\n",
    "            \n",
    "            target_mask = (target == target_digit)\n",
    "            other_mask = (target != target_digit)\n",
    "            \n",
    "            if target_mask.sum() > 0:\n",
    "                target_activations.append(activations[target_mask])\n",
    "            if other_mask.sum() > 0:\n",
    "                other_activations.append(activations[other_mask])\n",
    "    \n",
    "    target_activations = torch.cat(target_activations, dim=0)\n",
    "    other_activations = torch.cat(other_activations, dim=0)\n",
    "    \n",
    "    # Compute correlations for target digit vs others\n",
    "    target_corr = torch.corrcoef(target_activations.T).cpu().numpy()\n",
    "    other_corr = torch.corrcoef(other_activations.T).cpu().numpy()\n",
    "    \n",
    "    # Find neurons with different correlation patterns\n",
    "    correlation_diff = np.abs(target_corr - other_corr)\n",
    "    \n",
    "    # Visualize differences\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    # Target digit correlations\n",
    "    sns.heatmap(target_corr, cmap='RdBu_r', center=0, \n",
    "                vmin=-1, vmax=1, ax=axes[0], cbar=True)\n",
    "    axes[0].set_title(f'Neuron Correlations for Digit {target_digit}')\n",
    "    \n",
    "    # Other digits correlations\n",
    "    sns.heatmap(other_corr, cmap='RdBu_r', center=0, \n",
    "                vmin=-1, vmax=1, ax=axes[1], cbar=True)\n",
    "    axes[1].set_title('Neuron Correlations for Other Digits')\n",
    "    \n",
    "    # Correlation differences\n",
    "    sns.heatmap(correlation_diff, cmap='viridis', \n",
    "                ax=axes[2], cbar=True)\n",
    "    axes[2].set_title('Correlation Differences')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Find most different neuron pairs\n",
    "    n_neurons = correlation_diff.shape[0]\n",
    "    diff_pairs = []\n",
    "    \n",
    "    for i in range(n_neurons):\n",
    "        for j in range(i+1, n_neurons):\n",
    "            diff_pairs.append((i, j, correlation_diff[i, j]))\n",
    "    \n",
    "    diff_pairs.sort(key=lambda x: x[2], reverse=True)\n",
    "    \n",
    "    print(f\"\\nTop 10 neuron pairs with most different correlations for digit {target_digit}:\")\n",
    "    for i, (n1, n2, diff) in enumerate(diff_pairs[:10]):\n",
    "        target_corr_val = target_corr[n1, n2]\n",
    "        other_corr_val = other_corr[n1, n2]\n",
    "        print(f\"{i+1}. Neurons {n1}-{n2}: Target corr = {target_corr_val:.3f}, Other corr = {other_corr_val:.3f}, Diff = {diff:.3f}\")\n",
    "    \n",
    "    return target_corr, other_corr, correlation_diff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f93f88",
   "metadata": {},
   "source": [
    "## Circuit Analysis: Information Flow Between Layers\n",
    "\n",
    "Analyze how information flows through the network layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97479845",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_information_flow(model, data_loader, max_samples=500):\n",
    "    \"\"\"Analyze how information flows between layers\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    layer_activations = {'fc1': [], 'fc2': [], 'fc3': [], 'fc4': []}\n",
    "    targets = []\n",
    "    \n",
    "    sample_count = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in data_loader:\n",
    "            if sample_count >= max_samples:\n",
    "                break\n",
    "            \n",
    "            all_activations = model.get_all_activations(data)\n",
    "            \n",
    "            for layer_name, activations in all_activations.items():\n",
    "                layer_activations[layer_name].append(activations)\n",
    "            \n",
    "            targets.append(target)\n",
    "            sample_count += data.size(0)\n",
    "    \n",
    "    # Concatenate all activations\n",
    "    for layer_name in layer_activations:\n",
    "        layer_activations[layer_name] = torch.cat(layer_activations[layer_name], dim=0)\n",
    "    \n",
    "    targets = torch.cat(targets, dim=0)\n",
    "    \n",
    "    # Compute layer-to-layer correlations\n",
    "    layer_names = ['fc1', 'fc2', 'fc3', 'fc4']\n",
    "    n_layers = len(layer_names)\n",
    "    \n",
    "    # For each pair of consecutive layers, compute cross-correlations\n",
    "    fig, axes = plt.subplots(1, n_layers-1, figsize=(15, 4))\n",
    "    \n",
    "    for i in range(n_layers-1):\n",
    "        layer1_name = layer_names[i]\n",
    "        layer2_name = layer_names[i+1]\n",
    "        \n",
    "        # Sample subset of neurons for computational efficiency\n",
    "        layer1_acts = layer_activations[layer1_name][:, :50]  # First 50 neurons\n",
    "        layer2_acts = layer_activations[layer2_name][:, :50]  # First 50 neurons\n",
    "        \n",
    "        # Compute cross-correlation matrix\n",
    "        combined = torch.cat([layer1_acts, layer2_acts], dim=1)\n",
    "        cross_corr = torch.corrcoef(combined.T).cpu().numpy()\n",
    "        \n",
    "        # Extract the cross-correlation part\n",
    "        n1 = layer1_acts.size(1)\n",
    "        n2 = layer2_acts.size(1)\n",
    "        cross_corr_subset = cross_corr[:n1, n1:n1+n2]\n",
    "        \n",
    "        # Plot\n",
    "        sns.heatmap(cross_corr_subset, cmap='RdBu_r', center=0,\n",
    "                   ax=axes[i], cbar=True)\n",
    "        axes[i].set_title(f'{layer1_name} â†’ {layer2_name}')\n",
    "        axes[i].set_xlabel(f'{layer2_name} neurons')\n",
    "        axes[i].set_ylabel(f'{layer1_name} neurons')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return layer_activations\n",
    "\n",
    "def analyze_layer_representations(layer_activations, targets):\n",
    "    \"\"\"Analyze how digit representations evolve across layers\"\"\"\n",
    "    layer_names = ['fc1', 'fc2', 'fc3', 'fc4']\n",
    "    \n",
    "    # Compute digit separability in each layer using PCA\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, layer_name in enumerate(layer_names):\n",
    "        activations = layer_activations[layer_name].cpu().numpy()\n",
    "        \n",
    "        # Apply PCA\n",
    "        pca = PCA(n_components=2)\n",
    "        activations_2d = pca.fit_transform(activations)\n",
    "        \n",
    "        # Plot different digits\n",
    "        colors = plt.cm.tab10(np.linspace(0, 1, 10))\n",
    "        for digit in range(10):\n",
    "            mask = targets.cpu().numpy() == digit\n",
    "            if mask.sum() > 0:\n",
    "                axes[i].scatter(activations_2d[mask, 0], activations_2d[mask, 1],\n",
    "                              c=[colors[digit]], label=f'{digit}', alpha=0.6, s=20)\n",
    "        \n",
    "        axes[i].set_title(f'{layer_name} Representations\\n'\n",
    "                         f'PC1: {pca.explained_variance_ratio_[0]:.2%}, '\n",
    "                         f'PC2: {pca.explained_variance_ratio_[1]:.2%}')\n",
    "        axes[i].set_xlabel('PC1')\n",
    "        axes[i].set_ylabel('PC2')\n",
    "        if i == 0:\n",
    "            axes[i].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5121ba",
   "metadata": {},
   "source": [
    "## Run Mechanistic Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6146c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model = MNISTNet()\n",
    "print(\"Training model...\")\n",
    "train_model(model, train_loader, epochs=5)\n",
    "\n",
    "# Test model accuracy\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        output = model(data)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target).sum().item()\n",
    "\n",
    "print(f'Test Accuracy: {100 * correct / total:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580d3c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze neuron correlations\n",
    "print(\"Analyzing neuron correlations...\")\n",
    "correlation_matrix, all_activations = analyze_neuron_correlations(model, test_loader, layer_name='fc2')\n",
    "high_corr_pairs = find_neuron_clusters(correlation_matrix, threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42631aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster neurons by activity patterns\n",
    "print(\"Clustering neurons by activity patterns...\")\n",
    "cluster_labels, kmeans = cluster_neurons_by_activity(all_activations, n_clusters=6)\n",
    "pca = visualize_neuron_clusters(all_activations, cluster_labels, n_clusters=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e329cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze cluster specialization\n",
    "print(\"Analyzing cluster specialization...\")\n",
    "cluster_specialization = analyze_cluster_specialization(model, test_loader, cluster_labels, layer_name='fc2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54367c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze neuron interactions for specific digit\n",
    "print(\"Analyzing neuron interactions for digit 7...\")\n",
    "target_corr, other_corr, correlation_diff = analyze_neuron_interactions(model, test_loader, layer_name='fc2', target_digit=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df50fb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze information flow between layers\n",
    "print(\"Analyzing information flow between layers...\")\n",
    "layer_activations = analyze_information_flow(model, test_loader, max_samples=1000)\n",
    "\n",
    "# Get targets for representation analysis\n",
    "targets = []\n",
    "sample_count = 0\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        if sample_count >= 1000:\n",
    "            break\n",
    "        targets.append(target)\n",
    "        sample_count += data.size(0)\n",
    "targets = torch.cat(targets, dim=0)[:1000]  # Limit to match layer_activations\n",
    "\n",
    "analyze_layer_representations(layer_activations, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e501a81",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrates several mechanistic interpretability techniques:\n",
    "\n",
    "1. **Neuron Correlation Analysis**: Understanding which neurons tend to activate together\n",
    "2. **Functional Clustering**: Grouping neurons by their activity patterns to identify functional modules\n",
    "3. **Cluster Specialization**: Analyzing what computational roles different neuron groups play\n",
    "4. **Task-Specific Interactions**: How neuron relationships change for different classification tasks\n",
    "5. **Information Flow**: How representations evolve as information flows through network layers\n",
    "\n",
    "These analyses help us understand:\n",
    "- How the network organizes its computation\n",
    "- Which neurons work together vs. independently\n",
    "- How different layers transform the input representations\n",
    "- What computational circuits exist for specific tasks\n",
    "\n",
    "This goes beyond simple \"what does each neuron do\" to \"how does the network compute\" - the core goal of mechanistic interpretability."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
