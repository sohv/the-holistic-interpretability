{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd9f1cf3",
   "metadata": {},
   "source": [
    "# Gradient-Based Attribution\n",
    "\n",
    "This notebook demonstrates gradient-based interpretability techniques to understand which input features are most important for model predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b62797",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcb4d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNISTNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c18bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST data\n",
    "transform = transforms.ToTensor()\n",
    "train_set = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_set = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b608e349",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, epochs=5):\n",
    "    \"\"\"Train the MNIST model\"\"\"\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{epochs}, Average Loss: {total_loss/len(train_loader):.4f}')\n",
    "    \n",
    "    print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a47bde6",
   "metadata": {},
   "source": [
    "## Simple Gradient Attribution\n",
    "\n",
    "Compute gradients of the output with respect to the input to see which pixels are most important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431250c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_input_gradients(model, input_tensor, target_class):\n",
    "    \"\"\"Compute gradients of output w.r.t. input to see important pixels\"\"\"\n",
    "    input_tensor.requires_grad_(True)\n",
    "    \n",
    "    output = model(input_tensor)\n",
    "    loss = output[0, target_class]\n",
    "    loss.backward()\n",
    "    \n",
    "    gradients = input_tensor.grad.data.abs()\n",
    "    return gradients\n",
    "\n",
    "def visualize_gradient_attribution(model, test_loader, num_samples=5):\n",
    "    \"\"\"Visualize which pixels are most important for predictions\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    fig, axes = plt.subplots(num_samples, 3, figsize=(12, 4*num_samples))\n",
    "    if num_samples == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    sample_count = 0\n",
    "    for data, target in test_loader:\n",
    "        if sample_count >= num_samples:\n",
    "            break\n",
    "            \n",
    "        for i in range(min(data.size(0), num_samples - sample_count)):\n",
    "            img = data[i:i+1]\n",
    "            true_label = target[i].item()\n",
    "            \n",
    "            # Get prediction\n",
    "            with torch.no_grad():\n",
    "                pred = model(img).argmax().item()\n",
    "            \n",
    "            # Compute gradients\n",
    "            gradients = compute_input_gradients(model, img.clone(), pred)\n",
    "            \n",
    "            # Plot original, gradients, and overlay\n",
    "            img_np = img.squeeze().cpu().numpy()\n",
    "            grad_np = gradients.squeeze().cpu().numpy()\n",
    "            \n",
    "            row = sample_count + i\n",
    "            \n",
    "            axes[row, 0].imshow(img_np, cmap='gray')\n",
    "            axes[row, 0].set_title(f'Original (True: {true_label}, Pred: {pred})')\n",
    "            axes[row, 0].axis('off')\n",
    "            \n",
    "            axes[row, 1].imshow(grad_np, cmap='hot')\n",
    "            axes[row, 1].set_title('Gradient Magnitude')\n",
    "            axes[row, 1].axis('off')\n",
    "            \n",
    "            axes[row, 2].imshow(img_np, cmap='gray', alpha=0.7)\n",
    "            axes[row, 2].imshow(grad_np, cmap='hot', alpha=0.3)\n",
    "            axes[row, 2].set_title('Overlay')\n",
    "            axes[row, 2].axis('off')\n",
    "        \n",
    "        sample_count += min(data.size(0), num_samples - sample_count)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8606ccca",
   "metadata": {},
   "source": [
    "## Integrated Gradients\n",
    "\n",
    "A more sophisticated attribution method that integrates gradients along a path from a baseline to the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1539a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def integrated_gradients(model, input_tensor, target_class, baseline=None, steps=50):\n",
    "    \"\"\"Compute integrated gradients for better attribution\"\"\"\n",
    "    if baseline is None:\n",
    "        baseline = torch.zeros_like(input_tensor)\n",
    "    \n",
    "    # Generate path from baseline to input\n",
    "    alphas = torch.linspace(0, 1, steps)\n",
    "    \n",
    "    integrated_grads = torch.zeros_like(input_tensor)\n",
    "    \n",
    "    for alpha in alphas:\n",
    "        # Interpolate between baseline and input\n",
    "        interpolated = baseline + alpha * (input_tensor - baseline)\n",
    "        interpolated.requires_grad_(True)\n",
    "        \n",
    "        # Compute gradients\n",
    "        output = model(interpolated)\n",
    "        loss = output[0, target_class]\n",
    "        \n",
    "        gradients = torch.autograd.grad(loss, interpolated, create_graph=False)[0]\n",
    "        integrated_grads += gradients\n",
    "    \n",
    "    # Average the gradients and multiply by (input - baseline)\n",
    "    integrated_grads = integrated_grads / steps\n",
    "    integrated_grads = integrated_grads * (input_tensor - baseline)\n",
    "    \n",
    "    return integrated_grads\n",
    "\n",
    "def visualize_integrated_gradients(model, test_loader, num_samples=3):\n",
    "    \"\"\"Visualize integrated gradients attribution\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    fig, axes = plt.subplots(num_samples, 4, figsize=(16, 4*num_samples))\n",
    "    if num_samples == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    sample_count = 0\n",
    "    for data, target in test_loader:\n",
    "        if sample_count >= num_samples:\n",
    "            break\n",
    "            \n",
    "        for i in range(min(data.size(0), num_samples - sample_count)):\n",
    "            img = data[i:i+1]\n",
    "            true_label = target[i].item()\n",
    "            \n",
    "            # Get prediction\n",
    "            with torch.no_grad():\n",
    "                pred = model(img).argmax().item()\n",
    "            \n",
    "            # Compute simple gradients\n",
    "            simple_grads = compute_input_gradients(model, img.clone(), pred)\n",
    "            \n",
    "            # Compute integrated gradients\n",
    "            ig_attr = integrated_gradients(model, img.clone(), pred, steps=30)\n",
    "            \n",
    "            # Plot comparisons\n",
    "            img_np = img.squeeze().cpu().numpy()\n",
    "            simple_grad_np = simple_grads.squeeze().cpu().numpy()\n",
    "            ig_attr_np = ig_attr.squeeze().cpu().numpy().abs()\n",
    "            \n",
    "            row = sample_count + i\n",
    "            \n",
    "            axes[row, 0].imshow(img_np, cmap='gray')\n",
    "            axes[row, 0].set_title(f'Original (True: {true_label}, Pred: {pred})')\n",
    "            axes[row, 0].axis('off')\n",
    "            \n",
    "            axes[row, 1].imshow(simple_grad_np, cmap='hot')\n",
    "            axes[row, 1].set_title('Simple Gradients')\n",
    "            axes[row, 1].axis('off')\n",
    "            \n",
    "            axes[row, 2].imshow(ig_attr_np, cmap='hot')\n",
    "            axes[row, 2].set_title('Integrated Gradients')\n",
    "            axes[row, 2].axis('off')\n",
    "            \n",
    "            # Overlay integrated gradients\n",
    "            axes[row, 3].imshow(img_np, cmap='gray', alpha=0.7)\n",
    "            axes[row, 3].imshow(ig_attr_np, cmap='hot', alpha=0.3)\n",
    "            axes[row, 3].set_title('IG Overlay')\n",
    "            axes[row, 3].axis('off')\n",
    "        \n",
    "        sample_count += min(data.size(0), num_samples - sample_count)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309c68ae",
   "metadata": {},
   "source": [
    "## Guided Backpropagation\n",
    "\n",
    "A technique that modifies the backward pass to only propagate positive gradients through ReLU layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a991ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GuidedBackpropReLU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GuidedBackpropReLU, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.relu(x)\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        # Only pass positive gradients\n",
    "        return grad_output * (grad_output > 0).float()\n",
    "\n",
    "def guided_backprop(model, input_tensor, target_class):\n",
    "    \"\"\"Perform guided backpropagation\"\"\"\n",
    "    # Create a copy of the model with guided ReLUs\n",
    "    guided_model = type(model)()\n",
    "    guided_model.load_state_dict(model.state_dict())\n",
    "    \n",
    "    # Replace ReLU activations (this is simplified for the fully connected model)\n",
    "    input_tensor.requires_grad_(True)\n",
    "    \n",
    "    # Forward pass\n",
    "    output = guided_model(input_tensor)\n",
    "    loss = output[0, target_class]\n",
    "    \n",
    "    # Backward pass with guided gradients\n",
    "    gradients = torch.autograd.grad(loss, input_tensor, create_graph=False)[0]\n",
    "    \n",
    "    # Apply guided backprop rule: keep only positive gradients\n",
    "    guided_gradients = gradients * (gradients > 0).float()\n",
    "    \n",
    "    return guided_gradients\n",
    "\n",
    "def compare_attribution_methods(model, test_loader, num_samples=2):\n",
    "    \"\"\"Compare different attribution methods side by side\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    fig, axes = plt.subplots(num_samples, 5, figsize=(20, 4*num_samples))\n",
    "    if num_samples == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    sample_count = 0\n",
    "    for data, target in test_loader:\n",
    "        if sample_count >= num_samples:\n",
    "            break\n",
    "            \n",
    "        for i in range(min(data.size(0), num_samples - sample_count)):\n",
    "            img = data[i:i+1]\n",
    "            true_label = target[i].item()\n",
    "            \n",
    "            # Get prediction\n",
    "            with torch.no_grad():\n",
    "                pred = model(img).argmax().item()\n",
    "            \n",
    "            # Skip misclassified examples\n",
    "            if pred != true_label:\n",
    "                continue\n",
    "            \n",
    "            # Compute different attributions\n",
    "            simple_grads = compute_input_gradients(model, img.clone(), pred)\n",
    "            ig_attr = integrated_gradients(model, img.clone(), pred, steps=20)\n",
    "            guided_grads = guided_backprop(model, img.clone(), pred)\n",
    "            \n",
    "            # Convert to numpy\n",
    "            img_np = img.squeeze().cpu().numpy()\n",
    "            simple_grad_np = simple_grads.squeeze().cpu().numpy()\n",
    "            ig_attr_np = ig_attr.squeeze().cpu().numpy().abs()\n",
    "            guided_grad_np = guided_grads.squeeze().cpu().numpy().abs()\n",
    "            \n",
    "            row = sample_count + i\n",
    "            \n",
    "            axes[row, 0].imshow(img_np, cmap='gray')\n",
    "            axes[row, 0].set_title(f'Original\\n(Digit {true_label})')\n",
    "            axes[row, 0].axis('off')\n",
    "            \n",
    "            axes[row, 1].imshow(simple_grad_np, cmap='hot')\n",
    "            axes[row, 1].set_title('Simple\\nGradients')\n",
    "            axes[row, 1].axis('off')\n",
    "            \n",
    "            axes[row, 2].imshow(ig_attr_np, cmap='hot')\n",
    "            axes[row, 2].set_title('Integrated\\nGradients')\n",
    "            axes[row, 2].axis('off')\n",
    "            \n",
    "            axes[row, 3].imshow(guided_grad_np, cmap='hot')\n",
    "            axes[row, 3].set_title('Guided\\nBackprop')\n",
    "            axes[row, 3].axis('off')\n",
    "            \n",
    "            # Best overlay (using integrated gradients)\n",
    "            axes[row, 4].imshow(img_np, cmap='gray', alpha=0.7)\n",
    "            axes[row, 4].imshow(ig_attr_np, cmap='hot', alpha=0.4)\n",
    "            axes[row, 4].set_title('IG Overlay')\n",
    "            axes[row, 4].axis('off')\n",
    "        \n",
    "        sample_count += min(data.size(0), num_samples - sample_count)\n",
    "        if sample_count >= num_samples:\n",
    "            break\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45812ec2",
   "metadata": {},
   "source": [
    "## Run Gradient Attribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692b9ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model = MNISTNet()\n",
    "print(\"Training model...\")\n",
    "train_model(model, train_loader, epochs=5)\n",
    "\n",
    "# Test model accuracy\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        output = model(data)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target).sum().item()\n",
    "\n",
    "print(f'Test Accuracy: {100 * correct / total:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38a2c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple gradient attribution\n",
    "print(\"Simple Gradient Attribution:\")\n",
    "visualize_gradient_attribution(model, test_loader, num_samples=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3f66d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integrated gradients\n",
    "print(\"Integrated Gradients vs Simple Gradients:\")\n",
    "visualize_integrated_gradients(model, test_loader, num_samples=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b66cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all methods\n",
    "print(\"Comparison of Attribution Methods:\")\n",
    "compare_attribution_methods(model, test_loader, num_samples=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565a7fdc",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrates several gradient-based attribution methods:\n",
    "\n",
    "1. **Simple Gradients**: Basic gradient of output w.r.t. input\n",
    "   - Fast and simple\n",
    "   - Can be noisy and misleading\n",
    "\n",
    "2. **Integrated Gradients**: Integrates gradients along path from baseline\n",
    "   - More stable and reliable\n",
    "   - Satisfies important axioms (sensitivity, implementation invariance)\n",
    "   - Better handles saturation effects\n",
    "\n",
    "3. **Guided Backpropagation**: Modified backward pass through ReLUs\n",
    "   - Focuses on positive contributions\n",
    "   - Can produce cleaner visualizations\n",
    "\n",
    "These methods help us understand which input features (pixels) the model considers most important for making specific predictions."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
